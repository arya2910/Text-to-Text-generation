{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOp65Spe6ZUdtTfEihlOxw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arya2910/Text-to-Text-generation/blob/main/Copy_of_texttotext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP-NQ7kDydUX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/game_of_thrones.txt\", \"r\") as f:\n",
        "  text=f.readlines()\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "Eeke66l45cUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters and convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    # Join tokens into a string\n",
        "    text = ' '.join(tokens)\n",
        "    return text"
      ],
      "metadata": {
        "id": "54HoQ6awv8rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = []\n",
        "for i in range(len(text)):\n",
        "  if text[i]!='\\n':\n",
        "    cleaned_data.append(clean_text(text[i]))"
      ],
      "metadata": {
        "id": "2W25Oae3v-KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_data)"
      ],
      "metadata": {
        "id": "J1AMxTsc6MCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "gZSZrwO623XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "#tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "#tokenizer.fit_on_texts([text])\n",
        "#encoded_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(cleaned_data)\n",
        "encoded_text = tokenizer.texts_to_sequences(cleaned_data)\n",
        "print(encoded_text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "4L1oN-5ACgJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(encoded_text, maxlen=70, padding='post')\n",
        "print(padded_sequences.shape)\n",
        "embedded_sequences = padded_sequences.reshape((padded_sequences.shape[0], padded_sequences.shape[1], 1))"
      ],
      "metadata": {
        "id": "VjrP4IITCxqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume that you have an unsupervised dataset called 'data'\n",
        "\n",
        "# Split the data into train and test sets with a 80/20 split\n",
        "train_data, test_data = train_test_split(padded_sequences, test_size=0.2)"
      ],
      "metadata": {
        "id": "3R8OaxA7lA0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 128, input_length=69))\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(Dense(len(tokenizer.word_index)  +1, activation='softmax'))\n",
        "\n",
        "# Compile RNN model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'],)\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(train_data[:, :-1], tf.keras.utils.to_categorical(train_data[:, 1:]), epochs=100)"
      ],
      "metadata": {
        "id": "Wd40KtEVCXnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "seed_text = \"The quick brown fox\"\n",
        "seq_len=10         jlm/''\n",
        "for i in range(10):\n",
        "    encoded_seed = tokenizer.texts_to_sequences([seed_text])[0][-seq_len:]\n",
        "    encoded_seed = tf.keras.utils.to_categorical(encoded_seed, num_classes=len(tokenizer.word_index)+1)\n",
        "    next_word_one_hot = model.predict(tf.expand_dims(encoded_seed, 0))[-1]\n",
        "    next_word_index = tf.argmax(next_word_one_hot).numpy()\n",
        "    next_words = [word for word, index in tokenizer.word_index.items() if index == next_word_index]\n",
        "    next_word = random.choice(next_words)\n",
        "    seed_text += ' ' + next_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "id": "taYLHkVhD8-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(lentest_data)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "uctPr1KykI3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "sentences = tokenized_data # assuming clean_text is defined beforehand\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=70, padding='post')\n",
        "print(len(padded_sequences))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yWAvC08qeZzZ"
      }
    }
  ]
}